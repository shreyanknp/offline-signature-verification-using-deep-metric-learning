{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15839b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG exists: True W:\\SRH study\\Case Study 2\\Offline Signature Verification\\Datasets\\signatures\\full_org\n",
      "FORG exists: True W:\\SRH study\\Case Study 2\\Offline Signature Verification\\Datasets\\signatures\\full_forg\n",
      "\n",
      "Total rows: 2640\n",
      "Bad filenames: 0\n",
      "\n",
      "Label counts:\n",
      "label\n",
      "genuine    1320\n",
      "forgery    1320\n",
      "Name: count, dtype: int64\n",
      "\n",
      "valid columns: ['writer_id', 'sample_id', 'label', 'path', 'filename_ok']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>writer_id</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>filename_ok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>genuine</td>\n",
       "      <td>W:\\SRH study\\Case Study 2\\Offline Signature Ve...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>genuine</td>\n",
       "      <td>W:\\SRH study\\Case Study 2\\Offline Signature Ve...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>genuine</td>\n",
       "      <td>W:\\SRH study\\Case Study 2\\Offline Signature Ve...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>genuine</td>\n",
       "      <td>W:\\SRH study\\Case Study 2\\Offline Signature Ve...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>genuine</td>\n",
       "      <td>W:\\SRH study\\Case Study 2\\Offline Signature Ve...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   writer_id  sample_id    label  \\\n",
       "0         10          1  genuine   \n",
       "1         10         10  genuine   \n",
       "2         10         11  genuine   \n",
       "3         10         12  genuine   \n",
       "4         10         13  genuine   \n",
       "\n",
       "                                                path  filename_ok  \n",
       "0  W:\\SRH study\\Case Study 2\\Offline Signature Ve...         True  \n",
       "1  W:\\SRH study\\Case Study 2\\Offline Signature Ve...         True  \n",
       "2  W:\\SRH study\\Case Study 2\\Offline Signature Ve...         True  \n",
       "3  W:\\SRH study\\Case Study 2\\Offline Signature Ve...         True  \n",
       "4  W:\\SRH study\\Case Study 2\\Offline Signature Ve...         True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Point to your dataset root\n",
    "DATA_ROOT = Path(r\"W:\\SRH study\\Case Study 2\\Offline Signature Verification\\Datasets\\signatures\")\n",
    "ORG_DIR  = DATA_ROOT / \"full_org\"\n",
    "FORG_DIR = DATA_ROOT / \"full_forg\"\n",
    "\n",
    "print(\"ORG exists:\", ORG_DIR.exists(), ORG_DIR)\n",
    "print(\"FORG exists:\", FORG_DIR.exists(), FORG_DIR)\n",
    "\n",
    "# 2) Filename patterns\n",
    "PAT_ORG  = re.compile(r\"^original_(\\d+)_(\\d+)\\.png$\", re.IGNORECASE)\n",
    "PAT_FORG = re.compile(r\"^forgeries_(\\d+)_(\\d+)\\.png$\", re.IGNORECASE)\n",
    "\n",
    "rows = []\n",
    "\n",
    "def scan(folder: Path, label: str, pat: re.Pattern):\n",
    "    for fp in folder.iterdir():\n",
    "        if not fp.is_file():\n",
    "            continue\n",
    "        if fp.suffix.lower() != \".png\":     # handles .PNG as well\n",
    "            continue\n",
    "\n",
    "        m = pat.match(fp.name)\n",
    "        if not m:\n",
    "            # file exists but naming doesn't match expected pattern\n",
    "            rows.append({\n",
    "                \"writer_id\": None,\n",
    "                \"sample_id\": None,\n",
    "                \"label\": label,\n",
    "                \"path\": str(fp),\n",
    "                \"filename_ok\": False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"writer_id\": int(m.group(1)),\n",
    "            \"sample_id\": int(m.group(2)),\n",
    "            \"label\": label,\n",
    "            \"path\": str(fp),\n",
    "            \"filename_ok\": True\n",
    "        })\n",
    "\n",
    "scan(ORG_DIR,  \"genuine\", PAT_ORG)\n",
    "scan(FORG_DIR, \"forgery\", PAT_FORG)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\nTotal rows:\", len(df))\n",
    "print(\"Bad filenames:\", (~df[\"filename_ok\"]).sum())\n",
    "print(\"\\nLabel counts:\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# 3) This is the DataFrame you need for pair generation\n",
    "valid = df[df[\"filename_ok\"]].copy()\n",
    "\n",
    "# Optional: quick check\n",
    "print(\"\\nvalid columns:\", list(valid.columns))\n",
    "display(valid.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e84bdea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writers: 55\n",
      "Train writers: 38\n",
      "Val writers: 8\n",
      "Test writers: 9\n",
      "Overlap train-val: 0\n",
      "Overlap train-test: 0\n",
      "Overlap val-test: 0\n"
     ]
    }
   ],
   "source": [
    "# Writer-independent split (train/val/test)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_writers(valid_df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-9\n",
    "    \n",
    "    writers = np.array(sorted(valid_df[\"writer_id\"].unique()))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(writers)\n",
    "\n",
    "    n = len(writers)\n",
    "    n_train = int(round(n * train_ratio))\n",
    "    n_val   = int(round(n * val_ratio))\n",
    "    # remaining goes to test\n",
    "    train_w = writers[:n_train]\n",
    "    val_w   = writers[n_train:n_train+n_val]\n",
    "    test_w  = writers[n_train+n_val:]\n",
    "\n",
    "    return set(train_w), set(val_w), set(test_w)\n",
    "\n",
    "train_writers, val_writers, test_writers = split_writers(valid, seed=42)\n",
    "\n",
    "print(\"Writers:\", len(valid[\"writer_id\"].unique()))\n",
    "print(\"Train writers:\", len(train_writers))\n",
    "print(\"Val writers:\", len(val_writers))\n",
    "print(\"Test writers:\", len(test_writers))\n",
    "\n",
    "# sanity: no overlap\n",
    "print(\"Overlap train-val:\", len(train_writers & val_writers))\n",
    "print(\"Overlap train-test:\", len(train_writers & test_writers))\n",
    "print(\"Overlap val-test:\", len(val_writers & test_writers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe5b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: build per-writer pools (genuine / forgery lists)\n",
    "def build_pools(df_subset):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      genuine_by_writer: {writer_id: [paths...]}\n",
    "      forgery_by_writer: {writer_id: [paths...]}\n",
    "    \"\"\"\n",
    "    genuine_by_writer = {}\n",
    "    forgery_by_writer = {}\n",
    "    \n",
    "    for wid, group in df_subset.groupby(\"writer_id\"):\n",
    "        g_paths = group[group[\"label\"] == \"genuine\"][\"path\"].tolist()\n",
    "        f_paths = group[group[\"label\"] == \"forgery\"][\"path\"].tolist()\n",
    "        if len(g_paths) > 0:\n",
    "            genuine_by_writer[wid] = g_paths\n",
    "        if len(f_paths) > 0:\n",
    "            forgery_by_writer[wid] = f_paths\n",
    "    \n",
    "    return genuine_by_writer, forgery_by_writer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb30974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate balanced pairs\n",
    "def generate_pairs_for_writers(valid_df, writer_set, n_pairs=20000, seed=42, neg_mix=0.5):\n",
    "    \"\"\"\n",
    "    Generates a DataFrame with columns:\n",
    "      path_a, path_b, label, pair_type, writer_a, writer_b\n",
    "    \n",
    "    neg_mix: fraction of negatives that are same-writer (genuine vs forgery)\n",
    "             remaining negatives are cross-writer (genuine vs genuine from different writers)\n",
    "    \"\"\"\n",
    "    df_subset = valid_df[valid_df[\"writer_id\"].isin(writer_set)].copy()\n",
    "    genuine_by_writer, forgery_by_writer = build_pools(df_subset)\n",
    "    \n",
    "    writers = sorted(set(genuine_by_writer.keys()))\n",
    "    if len(writers) < 2:\n",
    "        raise ValueError(\"Need at least 2 writers with genuine samples to form cross-writer negatives.\")\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    pairs = []\n",
    "\n",
    "    n_pos = n_pairs // 2\n",
    "    n_neg = n_pairs - n_pos\n",
    "    n_neg_same = int(round(n_neg * neg_mix))\n",
    "    n_neg_cross = n_neg - n_neg_same\n",
    "\n",
    "    # -------- Positive pairs: genuine-genuine same writer --------\n",
    "    for _ in range(n_pos):\n",
    "        w = rng.choice(writers)\n",
    "        g_list = genuine_by_writer[w]\n",
    "        # pick two DIFFERENT genuine samples\n",
    "        a, b = rng.choice(len(g_list), size=2, replace=False)\n",
    "        pairs.append({\n",
    "            \"path_a\": g_list[a],\n",
    "            \"path_b\": g_list[b],\n",
    "            \"label\": 1,\n",
    "            \"pair_type\": \"pos_genuine_genuine\",\n",
    "            \"writer_a\": w,\n",
    "            \"writer_b\": w\n",
    "        })\n",
    "\n",
    "    # -------- Negative pairs 1: genuine-forgery same writer --------\n",
    "    writers_with_forg = sorted(set(genuine_by_writer.keys()) & set(forgery_by_writer.keys()))\n",
    "    if len(writers_with_forg) == 0:\n",
    "        raise ValueError(\"No writers have both genuine and forgery samples in this split.\")\n",
    "\n",
    "    for _ in range(n_neg_same):\n",
    "        w = rng.choice(writers_with_forg)\n",
    "        g_list = genuine_by_writer[w]\n",
    "        f_list = forgery_by_writer[w]\n",
    "        a = rng.integers(0, len(g_list))\n",
    "        b = rng.integers(0, len(f_list))\n",
    "        pairs.append({\n",
    "            \"path_a\": g_list[a],\n",
    "            \"path_b\": f_list[b],\n",
    "            \"label\": 0,\n",
    "            \"pair_type\": \"neg_genuine_forgery_same_writer\",\n",
    "            \"writer_a\": w,\n",
    "            \"writer_b\": w\n",
    "        })\n",
    "\n",
    "    # -------- Negative pairs 2: genuine-genuine different writers --------\n",
    "    for _ in range(n_neg_cross):\n",
    "        w1, w2 = rng.choice(writers, size=2, replace=False)\n",
    "        g1 = genuine_by_writer[w1]\n",
    "        g2 = genuine_by_writer[w2]\n",
    "        a = rng.integers(0, len(g1))\n",
    "        b = rng.integers(0, len(g2))\n",
    "        pairs.append({\n",
    "            \"path_a\": g1[a],\n",
    "            \"path_b\": g2[b],\n",
    "            \"label\": 0,\n",
    "            \"pair_type\": \"neg_genuine_genuine_cross_writer\",\n",
    "            \"writer_a\": w1,\n",
    "            \"writer_b\": w2\n",
    "        })\n",
    "\n",
    "    pairs_df = pd.DataFrame(pairs)\n",
    "    # shuffle rows\n",
    "    pairs_df = pairs_df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    return pairs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba8472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 40000\n",
      "Val pairs: 10000\n",
      "Test pairs: 10000\n",
      "\n",
      "Train label balance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.5\n",
       "0    0.5\n",
       "Name: fraction, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train pair types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pair_type\n",
       "pos_genuine_genuine                 20000\n",
       "neg_genuine_genuine_cross_writer    10000\n",
       "neg_genuine_forgery_same_writer     10000\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create train/val/test pairs\n",
    "train_pairs = generate_pairs_for_writers(valid, train_writers, n_pairs=40000, seed=1)\n",
    "val_pairs   = generate_pairs_for_writers(valid, val_writers,   n_pairs=10000, seed=2)\n",
    "test_pairs  = generate_pairs_for_writers(valid, test_writers,  n_pairs=10000, seed=3)\n",
    "\n",
    "print(\"Train pairs:\", len(train_pairs))\n",
    "print(\"Val pairs:\", len(val_pairs))\n",
    "print(\"Test pairs:\", len(test_pairs))\n",
    "\n",
    "print(\"\\nTrain label balance:\")\n",
    "display(train_pairs[\"label\"].value_counts(normalize=True).rename(\"fraction\"))\n",
    "\n",
    "print(\"\\nTrain pair types:\")\n",
    "display(train_pairs[\"pair_type\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de04d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive pairs with different writers (should be 0): 0\n",
      "Cross-writer negatives with same writer (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "# 1) Are positives truly same-writer?\n",
    "pos_bad = train_pairs[(train_pairs[\"label\"] == 1) & (train_pairs[\"writer_a\"] != train_pairs[\"writer_b\"])]\n",
    "print(\"Positive pairs with different writers (should be 0):\", len(pos_bad))\n",
    "\n",
    "# 2) Are cross-writer negatives truly different-writer?\n",
    "cross_bad = train_pairs[(train_pairs[\"pair_type\"] == \"neg_genuine_genuine_cross_writer\") & (train_pairs[\"writer_a\"] == train_pairs[\"writer_b\"])]\n",
    "print(\"Cross-writer negatives with same writer (should be 0):\", len(cross_bad))\n",
    "\n",
    "# 3) Ensure writer sets are disjoint (already checked earlier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c579f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Val overlap writers: 0\n",
      "Train-Test overlap writers: 0\n",
      "Val-Test overlap writers: 0\n"
     ]
    }
   ],
   "source": [
    "# check whether writer-independent split is truly disjoint\n",
    "train_ws = set(train_pairs[\"writer_a\"]).union(set(train_pairs[\"writer_b\"]))\n",
    "val_ws   = set(val_pairs[\"writer_a\"]).union(set(val_pairs[\"writer_b\"]))\n",
    "test_ws  = set(test_pairs[\"writer_a\"]).union(set(test_pairs[\"writer_b\"]))\n",
    "\n",
    "print(\"Train-Val overlap writers:\", len(train_ws & val_ws))\n",
    "print(\"Train-Test overlap writers:\", len(train_ws & test_ws))\n",
    "print(\"Val-Test overlap writers:\", len(val_ws & test_ws))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4341aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = (224, 224)  # you can change to 128x128 if training is slow\n",
    "\n",
    "def load_preprocess(path):\n",
    "    path = path.numpy().decode(\"utf-8\")\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.expand_dims(img, axis=-1)  # (H,W,1)\n",
    "    return img\n",
    "\n",
    "def tf_load_preprocess(path):\n",
    "    img = tf.py_function(load_preprocess, [path], Tout=tf.float32)\n",
    "    img.set_shape([IMG_SIZE[0], IMG_SIZE[1], 1])\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76895732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 1) (32, 224, 224, 1) (32,)\n"
     ]
    }
   ],
   "source": [
    "def make_pair_dataset(pairs_df, batch_size=32, shuffle=True):\n",
    "    a_paths = pairs_df[\"path_a\"].astype(str).values\n",
    "    b_paths = pairs_df[\"path_b\"].astype(str).values\n",
    "    labels  = pairs_df[\"label\"].astype(np.float32).values\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((a_paths, b_paths, labels))\n",
    "\n",
    "    def map_fn(a, b, y):\n",
    "        img_a = tf_load_preprocess(a)\n",
    "        img_b = tf_load_preprocess(b)\n",
    "        return (img_a, img_b), y\n",
    "\n",
    "    ds = ds.map(map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(2000, reshuffle_each_iteration=True)\n",
    "\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_pair_dataset(train_pairs, batch_size=32, shuffle=True)\n",
    "val_ds   = make_pair_dataset(val_pairs,   batch_size=32, shuffle=False)\n",
    "\n",
    "# sanity: inspect one batch\n",
    "(batch_imgs, batch_y) = next(iter(train_ds))\n",
    "print(batch_imgs[0].shape, batch_imgs[1].shape, batch_y.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
